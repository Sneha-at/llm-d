apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../base

patches:
- target:
    kind: Deployment
    name: llm-d-model-server
  patch: |-
    - op: replace
      path: /spec/template/spec/containers/0/image
      value: quay.io/oroz/llm-d:cpu-offloading-12
    - op: replace
      path: /spec/template/spec/containers/0/command
      value: 
        - "/bin/bash"
        - "-c"
    - op: replace
      path: /spec/template/spec/containers/0/args
      value:
        - >-
          mkdir -p /tmp/lmcache_prometheus &&
          vllm serve Qwen/Qwen3-32B
          --tensor-parallel-size 2
          --port 8000
          --max-num-seq 1024
          --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}'
          --enable_prefix_caching
          --enable-chunked-prefill
          --gpu-memory-utilization 0.65
    - op: add
      path: /spec/template/spec/containers/0/env/-
      value:
        name: LMCACHE_MAX_LOCAL_CPU_SIZE
        value: "200.0"
    - op: add
      path: /spec/template/spec/containers/0/env/-
      value:
        name: PYTHONHASHSEED
        value: "123"
    - op: add
      path: /spec/template/spec/containers/0/env/-
      value:
        name: PROMETHEUS_MULTIPROC_DIR
        value: "/tmp/lmcache_prometheus"
    - op: add
      path: /spec/template/spec/containers/0/resources
      value:
        limits:
          nvidia.com/gpu: 2
        requests:
          nvidia.com/gpu: 2
          memory: 400G  
